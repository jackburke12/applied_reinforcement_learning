{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GBod1nv65dN"
      },
      "source": [
        "## Lab: Cart Pole Demo 2 using OpenAI gym\n",
        "## Policy Gradient\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: March 4, 2024\n",
        "\n",
        "Jack Burke\n",
        "jpb2uj\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J73-oPOTqC5b"
      },
      "source": [
        "#### Instructions:  \n",
        "\n",
        "Carefully read the notes below and run the provided code. Answer each question clearly and show all results.\n",
        "\n",
        "#### TOTAL POINTS: 12\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6__XiTWDIiT"
      },
      "source": [
        "### First a Refresh\n",
        "\n",
        "Let's briefly review the content from Demo 1: Basics and Simple Policy\n",
        "\n",
        "We revisit the CartPole problem.\n",
        "\n",
        "We will work with the fork [gymnasium](https://gymnasium.farama.org/) which maintains OpenAI gym.  \n",
        "\n",
        "The *simple policy* didn't perform very well: the average reward was about 42.\n",
        "\n",
        "We want to see if we can do better using a Policy Gradient algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_dd0h6Eoap"
      },
      "source": [
        "### Setup and First Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wM5RiqAeqHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3ef96e-a689-4c99-da99-2037b17ddf14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4HpQPsAqH4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bb57ee-0265-4d64-fd05-fe2443245406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting renderlab\n",
            "  Downloading renderlab-0.1.20230421184216-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.0.3)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (0.0.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2025.10.5)\n",
            "Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: renderlab\n",
            "Successfully installed renderlab-0.1.20230421184216\n"
          ]
        }
      ],
      "source": [
        "! pip install renderlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ed_DyvHy6pbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5304b3-453e-4478-be73-712e0cfeefec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import renderlab as rl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA4JgtuL5jC9"
      },
      "source": [
        "Load the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcGLRi6F6vcQ"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "state = env.reset(seed=314)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQFJsxj7aBj"
      },
      "source": [
        "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
        "\n",
        "Components:   \n",
        "[0]: cart horizontal position (0.0 = center)  \n",
        "[1]: velocity (positive means right)  \n",
        "[2]: angle of the pole (0.0 = vertical)  \n",
        "[3]: pole's angular velocity (positive means clockwise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M30S6Bf_6yNv",
        "outputId": "39cc2ef5-2037-4d39-9da8-695837044fa9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.04225422, 0.02126478, 0.02520455, 0.00700802], dtype=float32), {})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A5peNKiBXsh",
        "outputId": "08eb1d32-e52d-4cdd-8c31-789ce6290a6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# state space number of components\n",
        "env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkZmsIN7ky6"
      },
      "source": [
        "The action space consists of two options:\n",
        "\n",
        "[0]: move cart left   \n",
        "[1]: move cart right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySQ8EpsV7Ngr",
        "outputId": "e8c36a11-8280-418d-8754-40641bfa0234"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_68lpUuVCc-_"
      },
      "source": [
        "Let's take an action, draw a sample and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKFEIHjL7n7h",
        "outputId": "b4b4fc87-2e57-4a7a-dad5-a95cea8b896e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state [ 0.04267951  0.21601637  0.02534471 -0.27761722]\n",
            "reward 1.0\n",
            "done False\n",
            "info {}\n"
          ]
        }
      ],
      "source": [
        "# move right\n",
        "action = 1\n",
        "\n",
        "# take a step and get next state, reward from environment\n",
        "state, reward, terminated, truncated, info = env.step(action)\n",
        "done = terminated or truncated\n",
        "\n",
        "print('state', state)\n",
        "print('reward', reward)\n",
        "print('done', done)\n",
        "print('info', info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqxI0vej88Kh"
      },
      "source": [
        "**Reward and Episode**  \n",
        "\n",
        "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
        "\n",
        "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
        "\n",
        "When the episode ends, a new episode may begin. The process learns cumulatively from each episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv5gKVde9pvQ"
      },
      "source": [
        "**Simple policy**:  \n",
        "\n",
        "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.\n",
        "\n",
        "Run many episodes and visualize their reward distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dREsiRKf9CbR"
      },
      "outputs": [],
      "source": [
        "def simple_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "num_episodes = 1000\n",
        "num_steps = 1000\n",
        "rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()[0]\n",
        "    for step in range(num_steps):\n",
        "        #print(state)\n",
        "        action = simple_policy(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards.append(ep_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "nzdjBfiA-yab",
        "outputId": "f0ded931-bf9b-48dd-cee5-1479f1c46092"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.boxplot(rewards, vert=False)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5H1zlxK-wb4",
        "outputId": "1dbe8489-74d3-4191-a51b-beeca58bee33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean reward: 42.213\n"
          ]
        }
      ],
      "source": [
        "print('mean reward:', np.mean(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2so9LpTSEb33"
      },
      "source": [
        "### Neural Network Policy\n",
        "\n",
        "Now we try a more sophisticated policy: let's use a neural network.\n",
        "\n",
        "The network will take **state as input**. The output node will contain the probability of the actions.\n",
        "\n",
        "Since there are two actions (left, right), we require one output node.  \n",
        "Node will output probability of right (so prob of left is implied).\n",
        "\n",
        "For simplicity, we will use one hidden layer.\n",
        "\n",
        "Number of nodes in hidden layer is a hyperparameter.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIb4dzbRrkzU"
      },
      "source": [
        "\n",
        "#### 1) **Define a neural network model for the policy.**\n",
        "\n",
        "**(POINTS: 1)**  \n",
        "It should have appropriate dimensions for the input and output layer. Print a summary of the model that shows the output shape for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZFXOXI_jCtgi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PolicyNN(nn.Module):\n",
        "  def __init__(self, hidden_size = 32):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(4, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PolicyNN(hidden_size=32)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6WS26g53kUC",
        "outputId": "ffdab890-30a8-4da8-f1b3-c665dc3fe31e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PolicyNN(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHftbUIR8YMf"
      },
      "source": [
        "#### 2) Is **REINFORCE** a Monte Carlo method? Explain your answer.\n",
        "\n",
        "**(POINTS: 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfkZsHku8Yh6"
      },
      "source": [
        "Yes, REINFORCE is a Monte Carlo method. It updates the policy only after an episode is completed, using the returns from that episode. Full-episode returns are calculated using Monte Carlo sampling, then the policy is updated when the episode ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmQEX73IA73"
      },
      "source": [
        "\n",
        "\n",
        "#### 3) **Training and Evaluating the Policy Model.**\n",
        "\n",
        "Define functions to train and evaluate the model. Your work should include the steps below among others. Note that subtasks are numbered [1], [2], etc. and are worth one point each.\n",
        "\n",
        "- **(POINTS: 2)** Write a function `play_single_step` to evolve the system one step. It should also compute the gradient of the loss function. Test that it works properly and print the next state.  \n",
        "\n",
        "- **(POINTS: 2)** Write a function that runs multiple episodes, calling the `play_single_step` function. It should also compute and store the reward and gradient for each time step of each episode. Test that it works properly and print the rewards from running two episodes each with five time steps.\n",
        "- **(POINTS: 3)** Define and run a training loop using the REINFORCE algorithm that [1] runs multiple episodes, [2] computes discounted rewards, and [3] updates the parameters with gradient ascent. Run a sufficient number of episodes and time steps to see an average reward of at least 75 in the `evaluate` step that follows. Show evidence that the training loop is working, such as printing the total discounted rewards per episode.\n",
        "- **(POINTS: 3)** Now that the model is trained, you can use it as the policy and evaluate performance. Write code that [1] applies the model as the policy, [2] runs 1000 episodes and [3] computes the minimum reward, average reward, and maximum reward across the episodes. Discuss how the average reward from REINFORCE compares to the average reward from the simple policy from Cart Pole lab 1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h5F4i77ECzUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b0a08a-5890-4693-a7d9-4a82bedaef2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.02326423  0.15534712 -0.00881387 -0.25060967]\n"
          ]
        }
      ],
      "source": [
        "# play_single_step\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Bernoulli\n",
        "\n",
        "def play_single_step(env, model, state):\n",
        "    state_t = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    prob_right = model(state_t)\n",
        "    m = Bernoulli(prob_right)\n",
        "    action = m.sample()\n",
        "\n",
        "    # Convert decimal probability to int action\n",
        "    action_int = int(action.item())\n",
        "\n",
        "    # Step the environment\n",
        "    next_state, reward, terminated, truncated, info = env.step(action_int)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    # Compute log-prob gradient\n",
        "    log_prob = m.log_prob(action)\n",
        "    log_prob_grad = log_prob\n",
        "\n",
        "    return next_state, reward, done, log_prob_grad\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state, _ = env.reset()\n",
        "\n",
        "next_state, reward, done, grad = play_single_step(env, PolicyNN(), state)\n",
        "print(next_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd3zHM6Y2ig2",
        "outputId": "cb3a818e-3472-4b56-94b7-92ab78e4b0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode rewards: [[1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0]]\n"
          ]
        }
      ],
      "source": [
        "# play_multiple_steps\n",
        "\n",
        "def run_episodes(env, model, num_episodes, max_steps=500):\n",
        "    all_episode_rewards = []\n",
        "    all_episode_log_probs = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            next_state, reward, done, log_prob_grad = play_single_step(env, model, state)\n",
        "\n",
        "            rewards.append(reward)\n",
        "            log_probs.append(log_prob_grad)\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        all_episode_rewards.append(rewards)\n",
        "        all_episode_log_probs.append(log_probs)\n",
        "\n",
        "    return all_episode_rewards, all_episode_log_probs\n",
        "\n",
        "rewards, logps = run_episodes(env, PolicyNN(), num_episodes=2, max_steps=5)\n",
        "print(\"Episode rewards:\", rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eSFl4j62ig2",
        "outputId": "0a4d127d-16c6-43d1-a8dd-c34553f33c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode    0 | Total Reward = 16.0\n",
            "Episode    1 | Total Reward = 26.0\n",
            "Episode    2 | Total Reward = 23.0\n",
            "Episode    3 | Total Reward = 14.0\n",
            "Episode    4 | Total Reward = 66.0\n",
            "Episode    5 | Total Reward = 24.0\n",
            "Episode    6 | Total Reward = 52.0\n",
            "Episode    7 | Total Reward = 12.0\n",
            "Episode    8 | Total Reward = 40.0\n",
            "Episode    9 | Total Reward = 24.0\n",
            "Episode   10 | Total Reward = 12.0\n",
            "Episode   11 | Total Reward = 24.0\n",
            "Episode   12 | Total Reward = 33.0\n",
            "Episode   13 | Total Reward = 27.0\n",
            "Episode   14 | Total Reward = 32.0\n",
            "Episode   15 | Total Reward = 22.0\n",
            "Episode   16 | Total Reward = 16.0\n",
            "Episode   17 | Total Reward = 23.0\n",
            "Episode   18 | Total Reward = 37.0\n",
            "Episode   19 | Total Reward = 36.0\n",
            "Episode   20 | Total Reward = 25.0\n",
            "Episode   21 | Total Reward = 24.0\n",
            "Episode   22 | Total Reward = 27.0\n",
            "Episode   23 | Total Reward = 46.0\n",
            "Episode   24 | Total Reward = 53.0\n",
            "Episode   25 | Total Reward = 101.0\n",
            "Episode   26 | Total Reward = 30.0\n",
            "Episode   27 | Total Reward = 22.0\n",
            "Episode   28 | Total Reward = 40.0\n",
            "Episode   29 | Total Reward = 34.0\n",
            "Episode   30 | Total Reward = 43.0\n",
            "Episode   31 | Total Reward = 20.0\n",
            "Episode   32 | Total Reward = 45.0\n",
            "Episode   33 | Total Reward = 37.0\n",
            "Episode   34 | Total Reward = 61.0\n",
            "Episode   35 | Total Reward = 36.0\n",
            "Episode   36 | Total Reward = 22.0\n",
            "Episode   37 | Total Reward = 31.0\n",
            "Episode   38 | Total Reward = 63.0\n",
            "Episode   39 | Total Reward = 50.0\n",
            "Episode   40 | Total Reward = 30.0\n",
            "Episode   41 | Total Reward = 16.0\n",
            "Episode   42 | Total Reward = 55.0\n",
            "Episode   43 | Total Reward = 24.0\n",
            "Episode   44 | Total Reward = 40.0\n",
            "Episode   45 | Total Reward = 49.0\n",
            "Episode   46 | Total Reward = 37.0\n",
            "Episode   47 | Total Reward = 16.0\n",
            "Episode   48 | Total Reward = 24.0\n",
            "Episode   49 | Total Reward = 60.0\n",
            "Episode   50 | Total Reward = 42.0\n",
            "Episode   51 | Total Reward = 84.0\n",
            "Episode   52 | Total Reward = 26.0\n",
            "Episode   53 | Total Reward = 58.0\n",
            "Episode   54 | Total Reward = 84.0\n",
            "Episode   55 | Total Reward = 28.0\n",
            "Episode   56 | Total Reward = 78.0\n",
            "Episode   57 | Total Reward = 75.0\n",
            "Episode   58 | Total Reward = 48.0\n",
            "Episode   59 | Total Reward = 44.0\n",
            "Episode   60 | Total Reward = 51.0\n",
            "Episode   61 | Total Reward = 60.0\n",
            "Episode   62 | Total Reward = 42.0\n",
            "Episode   63 | Total Reward = 66.0\n",
            "Episode   64 | Total Reward = 31.0\n",
            "Episode   65 | Total Reward = 31.0\n",
            "Episode   66 | Total Reward = 79.0\n",
            "Episode   67 | Total Reward = 51.0\n",
            "Episode   68 | Total Reward = 77.0\n",
            "Episode   69 | Total Reward = 46.0\n",
            "Episode   70 | Total Reward = 45.0\n",
            "Episode   71 | Total Reward = 42.0\n",
            "Episode   72 | Total Reward = 73.0\n",
            "Episode   73 | Total Reward = 26.0\n",
            "Episode   74 | Total Reward = 70.0\n",
            "Episode   75 | Total Reward = 72.0\n",
            "Episode   76 | Total Reward = 46.0\n",
            "Episode   77 | Total Reward = 55.0\n",
            "Episode   78 | Total Reward = 59.0\n",
            "Episode   79 | Total Reward = 34.0\n",
            "Episode   80 | Total Reward = 40.0\n",
            "Episode   81 | Total Reward = 66.0\n",
            "Episode   82 | Total Reward = 58.0\n",
            "Episode   83 | Total Reward = 32.0\n",
            "Episode   84 | Total Reward = 41.0\n",
            "Episode   85 | Total Reward = 131.0\n",
            "Episode   86 | Total Reward = 53.0\n",
            "Episode   87 | Total Reward = 34.0\n",
            "Episode   88 | Total Reward = 74.0\n",
            "Episode   89 | Total Reward = 61.0\n",
            "Episode   90 | Total Reward = 57.0\n",
            "Episode   91 | Total Reward = 61.0\n",
            "Episode   92 | Total Reward = 90.0\n",
            "Episode   93 | Total Reward = 64.0\n",
            "Episode   94 | Total Reward = 65.0\n",
            "Episode   95 | Total Reward = 143.0\n",
            "Episode   96 | Total Reward = 63.0\n",
            "Episode   97 | Total Reward = 46.0\n",
            "Episode   98 | Total Reward = 74.0\n",
            "Episode   99 | Total Reward = 62.0\n",
            "Episode  100 | Total Reward = 22.0\n",
            "Episode  101 | Total Reward = 82.0\n",
            "Episode  102 | Total Reward = 64.0\n",
            "Episode  103 | Total Reward = 72.0\n",
            "Episode  104 | Total Reward = 64.0\n",
            "Episode  105 | Total Reward = 131.0\n",
            "Episode  106 | Total Reward = 73.0\n",
            "Episode  107 | Total Reward = 93.0\n",
            "Episode  108 | Total Reward = 76.0\n",
            "Episode  109 | Total Reward = 38.0\n",
            "Episode  110 | Total Reward = 136.0\n",
            "Episode  111 | Total Reward = 43.0\n",
            "Episode  112 | Total Reward = 84.0\n",
            "Episode  113 | Total Reward = 66.0\n",
            "Episode  114 | Total Reward = 95.0\n",
            "Episode  115 | Total Reward = 65.0\n",
            "Episode  116 | Total Reward = 83.0\n",
            "Episode  117 | Total Reward = 105.0\n",
            "Episode  118 | Total Reward = 115.0\n",
            "Episode  119 | Total Reward = 51.0\n",
            "Episode  120 | Total Reward = 73.0\n",
            "Episode  121 | Total Reward = 121.0\n",
            "Episode  122 | Total Reward = 132.0\n",
            "Episode  123 | Total Reward = 84.0\n",
            "Episode  124 | Total Reward = 108.0\n",
            "Episode  125 | Total Reward = 75.0\n",
            "Episode  126 | Total Reward = 116.0\n",
            "Episode  127 | Total Reward = 63.0\n",
            "Episode  128 | Total Reward = 86.0\n",
            "Episode  129 | Total Reward = 85.0\n",
            "Episode  130 | Total Reward = 95.0\n",
            "Episode  131 | Total Reward = 52.0\n",
            "Episode  132 | Total Reward = 100.0\n",
            "Episode  133 | Total Reward = 218.0\n",
            "Stopping early — reached target performance.\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "\n",
        "def compute_discounted_rewards(rewards, gamma=0.99):\n",
        "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
        "    running = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        running = rewards[t] + gamma * running\n",
        "        discounted[t] = running\n",
        "    return discounted\n",
        "\n",
        "def train_reinforce(env, model, optimizer,\n",
        "                    num_episodes=500, max_steps=500, gamma=0.99, stop_early=False):\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Collect experience\n",
        "        ep_rewards, ep_log_probs = run_episodes(\n",
        "            env, model, num_episodes=1, max_steps=max_steps\n",
        "        )\n",
        "        rewards = ep_rewards[0]\n",
        "        log_probs = ep_log_probs[0]\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        discounted = compute_discounted_rewards(rewards, gamma)\n",
        "\n",
        "        discounted_t = torch.tensor(discounted, dtype=torch.float32)\n",
        "\n",
        "        # Policy gradient loss\n",
        "        loss = 0\n",
        "        for log_prob, Gt in zip(log_probs, discounted_t):\n",
        "            loss += -log_prob * Gt\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Episode {episode:4d} | \"\n",
        "              f\"Total Reward = {sum(rewards)}\")\n",
        "\n",
        "        # Stop early once performance is good\n",
        "        if stop_early and np.sum(rewards) >= 200:\n",
        "            print(\"Stopping early — reached target performance.\")\n",
        "            break\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = PolicyNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train_reinforce(env, model, optimizer, num_episodes = 200, stop_early=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(POINTS: 3) Now that the model is trained, you can use it as the policy and evaluate performance. Write code that [1] applies the model as the policy, [2] runs 1000 episodes and [3] computes the minimum reward, average reward, and maximum reward across the episodes. Discuss how the average reward from REINFORCE compares to the average reward from the simple policy from Cart Pole lab 1."
      ],
      "metadata": {
        "id": "kJhs4Wr-62pq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsM5zuz_2ig2",
        "outputId": "09fa4ea2-e2ac-454e-a1e0-092e9d4936b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "  Min reward: 26.0\n",
            "  Avg reward: 94.54\n",
            "  Max reward: 275.0\n"
          ]
        }
      ],
      "source": [
        "# evaluate policy performance\n",
        "def evaluate_policy(env, model, num_episodes=1000, max_steps=500):\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        total = 0\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            state_t = torch.tensor(state, dtype=torch.float32)\n",
        "            prob = model(state_t).item()\n",
        "            action = 1 if np.random.rand() < prob else 0\n",
        "\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(total)\n",
        "\n",
        "    return min(rewards), np.mean(rewards), max(rewards)\n",
        "\n",
        "min_r, avg_r, max_r = evaluate_policy(env, model)\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Min reward: {min_r}\")\n",
        "print(f\"  Avg reward: {avg_r:.2f}\")\n",
        "print(f\"  Max reward: {max_r}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model for just 133 episodes, until it reached an episode with a total reward greater than 200, its performance in the evaluation step gave an average reward of 94.54. This is significantly higher than the simple policy's average reward of 42, and even higher than the simple policy's maximum reward."
      ],
      "metadata": {
        "id": "-IetVVfJ8CVK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}