{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
      "metadata": {
        "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c"
      },
      "source": [
        "### Lab: Value Iteration in a Grid World\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: May 26, 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
      "metadata": {
        "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45"
      },
      "source": [
        "#### Instructions:\n",
        "\n",
        "Implement value iteration for a $4 \\times 3$ gridworld environment. This will measure the value of each state. A robot in this world can make discrete moves: one step up, down, left or right. These actions are deterministic, meaning that the action selected will be taken with probability 1. There is a terminal state with reward +1 in the bottom right corner. All other states have reward 0. The discount factor is 0.9. Use tolerance $\\theta=0.01$. Show all code and results.\n",
        "\n",
        "**Note**: Do not use libraries from `networkx`, `gym`, `gymnasium` when solving this problem.\n",
        "\n",
        "#### Total Points: 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5956322-1af2-4a18-b65c-d98dc08454c8",
      "metadata": {
        "id": "a5956322-1af2-4a18-b65c-d98dc08454c8"
      },
      "source": [
        "---\n",
        "\n",
        "#### 1) **(POINTS: 2)** As part of your solution, create a GridWorld class with these attributes:\n",
        "\n",
        "- `nrows` : number of rows in the grid\n",
        "- `ncols` : number of columns in the grid\n",
        "\n",
        "and these methods:\n",
        "\n",
        "- `value_iteration()` with behavior described in [2] below\n",
        "- `get_reward()` : given the agent row and column, return the reward\n",
        "\n",
        "The class may include additional attributes and methods as well.\n",
        "\n",
        "Create an instance using the class, and call `nrows`, `ncols`, and `get_reward()` to verify correctness.\n",
        "\n",
        "You will not be graded on the implementation of `value_iteration()` for this problem.\n",
        "\n",
        "#### 2) **(POINTS: 8)** Here, you will be graded on the implementation of `value_iteration()`.\n",
        "Call `value_iteration()` to calculate and return the value function array. For each sweep over the states, have the function print out the intermediate array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd213107-d461-431c-b1e1-d1d3c099976d",
      "metadata": {
        "id": "bd213107-d461-431c-b1e1-d1d3c099976d"
      },
      "source": [
        "#### Enter all code here (you may also use multiple cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72",
      "metadata": {
        "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "858c5f68-7b54-459e-95f3-59697cfa6d23",
      "metadata": {
        "id": "858c5f68-7b54-459e-95f3-59697cfa6d23"
      },
      "source": [
        "#### 1) Create and test the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21a43af-2be7-49d3-aec0-1934160bb61c",
      "metadata": {
        "id": "a21a43af-2be7-49d3-aec0-1934160bb61c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dfb15497-6f08-4f3e-abef-43099f05fba7",
      "metadata": {
        "id": "dfb15497-6f08-4f3e-abef-43099f05fba7"
      },
      "source": [
        "#### 2) Run value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
      "metadata": {
        "id": "922da11f-c12f-4535-8a8a-63cbcab3e736"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "486efc92-9a71-4956-b877-b6b7cef13031",
      "metadata": {
        "id": "486efc92-9a71-4956-b877-b6b7cef13031"
      },
      "source": [
        "#### 3) **(POINTS: 2)** Based on the value function: After the agent has moved right or down, does it ever make sense for it to backtrack (move up or left)? Explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e",
      "metadata": {
        "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}