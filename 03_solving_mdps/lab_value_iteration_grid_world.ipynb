{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
      "metadata": {
        "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c"
      },
      "source": [
        "### Lab: Value Iteration in a Grid World\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: May 26, 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
      "metadata": {
        "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45"
      },
      "source": [
        "#### Instructions:\n",
        "\n",
        "Implement value iteration for a $4 \\times 3$ gridworld environment. This will measure the value of each state. A robot in this world can make discrete moves: one step up, down, left or right. These actions are deterministic, meaning that the action selected will be taken with probability 1. There is a terminal state with reward +1 in the bottom right corner. All other states have reward 0. The discount factor is 0.9. Use tolerance $\\theta=0.01$. Show all code and results.\n",
        "\n",
        "**Note**: Do not use libraries from `networkx`, `gym`, `gymnasium` when solving this problem.\n",
        "\n",
        "#### Total Points: 12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "rows, cols = 4, 3\n",
        "\n",
        "terminal_state = (3,2)\n",
        "\n",
        "def reward(state):\n",
        "  if state == terminal_state:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0\n",
        "\n",
        "actions = {\"up\":(-1, 0), \"down\":(1, 0), \"left\":(0, -1), \"right\":(0, 1)}\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.01\n",
        "\n",
        "V = np.zeros((rows, cols))\n",
        "\n",
        "def is_terminal(state):\n",
        "  return state == terminal_state\n",
        "\n",
        "def get_next_state(state, action):\n",
        "  row, col = state\n",
        "  dr, dc = actions[action]\n",
        "  next_row, next_col = row + dr, col + dc\n",
        "\n",
        "  #if action is in bounds, return new state\n",
        "  if 0 <= next_row < rows and 0 <= next_col < cols:\n",
        "    return (next_row, next_col)\n",
        "\n",
        "  #if action goes out of bounds, stay in current state\n",
        "  return state\n",
        "\n",
        "def value_iteration(V):\n",
        "  i = 0\n",
        "  while True:\n",
        "    delta = 0\n",
        "    new_V = np.copy(V)\n",
        "    for row in range(rows):\n",
        "      for col in range(cols):\n",
        "        state = (row, col)\n",
        "        if is_terminal(state):\n",
        "          new_V[state] = 0.0\n",
        "          continue\n",
        "        values = []\n",
        "        for action in actions:\n",
        "          next_state = get_next_state(state, action)\n",
        "          values.append(reward(next_state) + gamma * V[next_state])\n",
        "        new_V[state] = max(values)\n",
        "        delta = max(delta, abs(new_V[state] - V[state]))\n",
        "    V = new_V\n",
        "    i += 1\n",
        "    if delta < theta:\n",
        "      break\n",
        "  print(f'Converged after {i} iterations.')\n",
        "  print(\"Value Function:\")\n",
        "  for row in range(rows):\n",
        "    row_vals = \" | \".join(f\"{V[row, col]:6.3f}\" for col in range(cols))\n",
        "    print(row_vals)\n",
        "\n",
        "value_iteration(V)"
      ],
      "metadata": {
        "id": "ZJxaN_P0TRRr",
        "outputId": "b2c8f07a-0ff1-4c95-f32f-a1ff5caadb8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZJxaN_P0TRRr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 6 iterations.\n",
            "Value Function:\n",
            " 0.656 |  0.729 |  0.810\n",
            " 0.729 |  0.810 |  0.900\n",
            " 0.810 |  0.900 |  1.000\n",
            " 0.900 |  1.000 |  0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5956322-1af2-4a18-b65c-d98dc08454c8",
      "metadata": {
        "id": "a5956322-1af2-4a18-b65c-d98dc08454c8"
      },
      "source": [
        "---\n",
        "\n",
        "#### 1) **(POINTS: 2)** As part of your solution, create a GridWorld class with these attributes:\n",
        "\n",
        "- `nrows` : number of rows in the grid\n",
        "- `ncols` : number of columns in the grid\n",
        "\n",
        "and these methods:\n",
        "\n",
        "- `value_iteration()` with behavior described in [2] below\n",
        "- `get_reward()` : given the agent row and column, return the reward\n",
        "\n",
        "The class may include additional attributes and methods as well.\n",
        "\n",
        "Create an instance using the class, and call `nrows`, `ncols`, and `get_reward()` to verify correctness.\n",
        "\n",
        "You will not be graded on the implementation of `value_iteration()` for this problem.\n",
        "\n",
        "#### 2) **(POINTS: 8)** Here, you will be graded on the implementation of `value_iteration()`.\n",
        "Call `value_iteration()` to calculate and return the value function array. For each sweep over the states, have the function print out the intermediate array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd213107-d461-431c-b1e1-d1d3c099976d",
      "metadata": {
        "id": "bd213107-d461-431c-b1e1-d1d3c099976d"
      },
      "source": [
        "#### Enter all code here (you may also use multiple cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72",
      "metadata": {
        "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "  def __init__(self, nrows, ncols, terminal_state = (3,2), gamma = 0.9, theta = 0.01):\n",
        "    self.nrows = nrows\n",
        "    self.ncols = ncols\n",
        "    self.terminal_state = terminal_state\n",
        "    self.gamma = gamma\n",
        "    self.theta = theta\n",
        "\n",
        "    self.actions = {\n",
        "        \"up\": (-1, 0),\n",
        "        \"down\": (1, 0),\n",
        "        \"left\": (0, -1),\n",
        "        \"right\": (0, 1)\n",
        "    }\n",
        "\n",
        "    self.V = np.zeros((nrows, ncols))\n",
        "\n",
        "  def get_reward(self, state):\n",
        "      if state == self.terminal_state:\n",
        "          return 1.0\n",
        "      return 0.0\n",
        "\n",
        "  def is_terminal(self, state):\n",
        "      return state == self.terminal_state\n",
        "\n",
        "  def get_next_state(self, state, action):\n",
        "      row, col = state\n",
        "      dr, dc = self.actions[action]\n",
        "      nr, nc = row + dr, col + dc\n",
        "      if 0 <= nr < self.nrows and 0 <= nc < self.ncols:\n",
        "          return (nr, nc)\n",
        "      return state\n",
        "\n",
        "  def value_iteration(self):\n",
        "    i = 0\n",
        "    while True:\n",
        "      delta = 0\n",
        "      new_V = np.copy(self.V)\n",
        "      for row in range(self.nrows):\n",
        "        for col in range(self.ncols):\n",
        "          state = (row, col)\n",
        "          if self.is_terminal(state):\n",
        "            new_V[state] = 0.0\n",
        "            continue\n",
        "          values = []\n",
        "          for action in self.actions:\n",
        "            next_state = self.get_next_state(state, action)\n",
        "            values.append(self.get_reward(next_state) + self.gamma * self.V[next_state])\n",
        "          new_V[state] = max(values)\n",
        "          delta = max(delta, abs(new_V[state] - self.V[state]))\n",
        "      self.V = new_V\n",
        "      i += 1\n",
        "      if delta < self.theta:\n",
        "        break\n",
        "    print(f'Converged after {i} iterations.')\n",
        "    print(\"Value Function:\")\n",
        "    for row in range(self.nrows):\n",
        "      row_vals = \" | \".join(f\"{self.V[row, col]:6.3f}\" for col in range(self.ncols))\n",
        "      print(row_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858c5f68-7b54-459e-95f3-59697cfa6d23",
      "metadata": {
        "id": "858c5f68-7b54-459e-95f3-59697cfa6d23"
      },
      "source": [
        "#### 1) Create and test the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a21a43af-2be7-49d3-aec0-1934160bb61c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a21a43af-2be7-49d3-aec0-1934160bb61c",
        "outputId": "c79e4369-467d-43bb-98f4-b961b1f74941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 4\n",
            "Cols: 3\n",
            "Reward at terminal (3,2): 1.0\n",
            "Reward at non-terminal (0,0): 0.0\n"
          ]
        }
      ],
      "source": [
        "env = GridWorld(4, 3)\n",
        "print(\"Rows:\", env.nrows)\n",
        "print(\"Cols:\", env.ncols)\n",
        "print(\"Reward at terminal (3,2):\", env.get_reward((3, 2)))\n",
        "print(\"Reward at non-terminal (0,0):\", env.get_reward((0, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb15497-6f08-4f3e-abef-43099f05fba7",
      "metadata": {
        "id": "dfb15497-6f08-4f3e-abef-43099f05fba7"
      },
      "source": [
        "#### 2) Run value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
        "outputId": "9666fc4a-2a47-4752-b784-47bbcf92cd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 6 iterations.\n",
            "Value Function:\n",
            " 0.656 |  0.729 |  0.810\n",
            " 0.729 |  0.810 |  0.900\n",
            " 0.810 |  0.900 |  1.000\n",
            " 0.900 |  1.000 |  0.000\n"
          ]
        }
      ],
      "source": [
        "env.value_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486efc92-9a71-4956-b877-b6b7cef13031",
      "metadata": {
        "id": "486efc92-9a71-4956-b877-b6b7cef13031"
      },
      "source": [
        "#### 3) **(POINTS: 2)** Based on the value function: After the agent has moved right or down, does it ever make sense for it to backtrack (move up or left)? Explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e",
      "metadata": {
        "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}