{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
      "metadata": {
        "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c"
      },
      "source": [
        "### Lab: Navigating the Lunar Lander with a Dueling Deep Q Network\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: February 27, 2024\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
      "metadata": {
        "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45"
      },
      "source": [
        "#### Instructions:\n",
        "\n",
        "You will work with the `LunarLander-v2` environment from `gymnasium` in this lab.  \n",
        "\n",
        "An overview of the environment can be found [here](https://gymnasium.farama.org/).  \n",
        "If you're curious about the source code, see [here](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py).\n",
        "\n",
        "Your mission will be to implement a dueling deep Q -network using PyTorch.  \n",
        "You might run this on Colab.\n",
        "\n",
        "There are a few specific tasks outlined below for you to solve.\n",
        "\n",
        "The bigger tasks will be to:\n",
        "\n",
        "- Show that the algorithm works to train the agent in the environment\n",
        "- Run episodes and show the results\n",
        "\n",
        "**Submission**  \n",
        "As you will likely have several files including this notebook, you can zip all files and submit.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285f77b4-736e-4287-a570-2ecb5158e1b5",
      "metadata": {
        "id": "285f77b4-736e-4287-a570-2ecb5158e1b5"
      },
      "source": [
        "![lunar](https://github.com/jackburke12/applied_reinforcement_learning/blob/main/07_deep_q_networks_extensions2/lunar_lander1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c322b08-1720-43e0-a3b0-0788fc33b6a7",
      "metadata": {
        "id": "7c322b08-1720-43e0-a3b0-0788fc33b6a7"
      },
      "source": [
        "#### TOTAL POINTS: 12\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaec7e91-2f9e-4bae-8605-6a7f0d46e8a9",
      "metadata": {
        "id": "aaec7e91-2f9e-4bae-8605-6a7f0d46e8a9"
      },
      "source": [
        "**Hint:** Modules you may need to install include:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e46143-14c0-45da-8268-2a4c7295ca0c",
      "metadata": {
        "id": "15e46143-14c0-45da-8268-2a4c7295ca0c"
      },
      "source": [
        "swig  \n",
        "gym[box2d]  \n",
        "gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53817399-20ed-42eb-ae9e-9b76369de8eb",
      "metadata": {
        "id": "53817399-20ed-42eb-ae9e-9b76369de8eb"
      },
      "source": [
        "#### 1) What is the penalty for crashing?  \n",
        "**(POINTS: 1)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0599cdf7-e5b8-4d62-8ac1-bd507f10084a",
      "metadata": {
        "id": "0599cdf7-e5b8-4d62-8ac1-bd507f10084a"
      },
      "source": [
        "-100 is the penalty for crashing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a304cd5-72e7-4f53-95d4-1cb9fcfb1d1f",
      "metadata": {
        "id": "7a304cd5-72e7-4f53-95d4-1cb9fcfb1d1f"
      },
      "source": [
        "#### 2) Set up the environment and run 2 steps by taking random actions.\n",
        "**(POINTS: 1)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "obs, info = env.reset()\n",
        "print(\"Initial Observation: \", obs)\n",
        "\n",
        "for step in range(2):\n",
        "    action = env.action_space.sample()\n",
        "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "    print(f\"Step {step+1}\")\n",
        "    print(\"Action:\", action)\n",
        "    print(\"Next obs:\", next_obs)\n",
        "    print(\"Reward:\", reward, \"Terminated:\", terminated, \"Truncated:\", truncated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHcWGe2ih23H",
        "outputId": "65a8c599-a322-45de-b5ac-4430c210c6f1"
      },
      "id": "ZHcWGe2ih23H",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Observation:  [-0.0053813   1.4043136  -0.5450865  -0.29364684  0.00624241  0.12347026\n",
            "  0.          0.        ]\n",
            "Step 1\n",
            "Action: 1\n",
            "Next obs: [-0.0108407   1.3971329  -0.55409133 -0.31919786  0.01429689  0.16110553\n",
            "  0.          0.        ]\n",
            "Reward: -2.151115201650667 Terminated: False Truncated: False\n",
            "Step 2\n",
            "Action: 0\n",
            "Next obs: [-0.01630039  1.3893526  -0.55411613 -0.3458882   0.02234662  0.16100982\n",
            "  0.          0.        ]\n",
            "Reward: -1.4076542823901264 Terminated: False Truncated: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6704924-c33d-4135-bd79-7ffef75e7aa9",
      "metadata": {
        "id": "e6704924-c33d-4135-bd79-7ffef75e7aa9"
      },
      "source": [
        "#### 3) Briefly discuss your approach to solving the problem  \n",
        "**(POINTS: 2)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment provides an 8-dimensional continuous state vector and 4 discrete actions: do nothing, fire left, fire main, and fire right. We'll create a dueling deep Q-network to land the module safely.\n",
        "\n",
        "Our network will consist of an 8-feature input layer that takes the state vector as input. Two connected layers will be split into a value stream, V(s), and advantage stream, A(s,a). This is the core of the dueling network architecture. The streams will then combine to form Q(s,a), the state-action value function.\n",
        "\n",
        "We will also use a replay buffer to store transitions, and sample from mini-batches for training. A target network will provide stable Q-targets and will be updated periodically from the online policy network.\n",
        "\n",
        "The train step will compute the temporal-difference loss and update network parameters via gradient descent."
      ],
      "metadata": {
        "id": "p9DPfA7Ti0hu"
      },
      "id": "p9DPfA7Ti0hu"
    },
    {
      "cell_type": "markdown",
      "id": "894d315f-ed95-4b9a-b6f5-53e1305d272a",
      "metadata": {
        "id": "894d315f-ed95-4b9a-b6f5-53e1305d272a"
      },
      "source": [
        "#### 4) Create supporting code files (`.py` format) to create the agent, train, and run episodes\n",
        "**(POINTS: 6)**\n",
        "\n",
        "Your code should include:\n",
        "\n",
        "- **(POINTS: 1)** A class for the dueling DQN agent\n",
        "- **(POINTS: 1)** An architecture with separate Value and Advantage streams\n",
        "- **(POINTS: 1)** A method called `forward()` for the forward pass of the algorithm\n",
        "- **(POINTS: 1)** A replay buffer\n",
        "- **(POINTS: 1)** A training function\n",
        "- **(POINTS: 1)** A function to run episodes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "YJ6A8Ry9lZSQ"
      },
      "id": "YJ6A8Ry9lZSQ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3039df88-158c-455d-9333-4a8fcd7d7ad2",
      "metadata": {
        "id": "3039df88-158c-455d-9333-4a8fcd7d7ad2"
      },
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(DuelingDQN, self).__init__()\n",
        "    self.shared = nn.Sequential(\n",
        "        nn.Linear(state_dim, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,128),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.value_stream = nn.Sequential(\n",
        "        nn.Linear(128,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,1)\n",
        "    )\n",
        "\n",
        "    self.advantage_stream = nn.Sequential(\n",
        "        nn.Linear(128,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, action_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.shared(state)\n",
        "    value = self.value_stream(x)\n",
        "    advantage = self.advantage_stream(x)\n",
        "    #combine value and advantage streams. An average of the advantage function is used, like the dueling q-network paper\n",
        "    q_values = value + (advantage - advantage.mean(dim=1, keepdim = True))\n",
        "    return q_values\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    batch = random.sample(self.buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "    return (\n",
        "        torch.FloatTensor(states),\n",
        "        torch.LongTensor(actions),\n",
        "        torch.FloatTensor(rewards),\n",
        "        torch.FloatTensor(next_states),\n",
        "        torch.FloatTensor(dones)\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "def train_step(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma):\n",
        "  if len(replay_buffer) < batch_size:\n",
        "    return None\n",
        "\n",
        "  states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "  q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    max_next_q = target_net(next_states).max(1)[0]\n",
        "    target = rewards + gamma * max_next_q * (1-dones)\n",
        "\n",
        "  loss = F.mse_loss(q_values, target)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()\n",
        "\n",
        "def run_episodes(\n",
        "    env_name = \"LunarLander-v3\",\n",
        "    num_episodes = 100,\n",
        "    gamma = 0.99,\n",
        "    lr = 1e-3,\n",
        "    batch_size = 64,\n",
        "    buffer_capacity = 100000,\n",
        "    epsilon_start = 1.0,\n",
        "    epsilon_end = 0.01,\n",
        "    epsilon_decay = 0.995,\n",
        "    target_update_freq = 10\n",
        "):\n",
        "\n",
        "  env = gym.make(env_name)\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  policy_net = DuelingDQN(state_dim, action_dim)\n",
        "  target_net = DuelingDQN(state_dim, action_dim)\n",
        "  target_net.load_state_dict(policy_net.state_dict())\n",
        "  target_net.eval()\n",
        "\n",
        "  optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
        "  replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "  epsilon = epsilon_start\n",
        "  rewards_history = []\n",
        "\n",
        "  for episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if np.random.rand() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = policy_net(state_tensor)\n",
        "        action = q_values.argmax().item()\n",
        "\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      replay_buffer.push(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      total_reward += reward\n",
        "\n",
        "      loss = train_step(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma)\n",
        "\n",
        "    if episode % target_update_freq == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    rewards_history.append(total_reward)\n",
        "    print(f\"Episode {episode+1}/{num_episodes} | Reward: {total_reward:.1f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "  env.close()\n",
        "  return rewards_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7259568c-0352-4271-94cf-28361e1cabec",
      "metadata": {
        "id": "7259568c-0352-4271-94cf-28361e1cabec"
      },
      "source": [
        "#### 5) Run the training and show evidence that the agent is learning.  \n",
        "\n",
        "For example, its average reward (score) should increase with more episodes.\n",
        "\n",
        "**(POINTS: 1 if successful)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fe3a90f5-4fe6-41fc-88a0-1d234672b3a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe3a90f5-4fe6-41fc-88a0-1d234672b3a4",
        "outputId": "1989a471-cb67-4215-ebc9-e266df670be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 | Reward: -158.4 | Epsilon: 0.995\n",
            "Episode 2/100 | Reward: -159.6 | Epsilon: 0.990\n",
            "Episode 3/100 | Reward: -129.1 | Epsilon: 0.985\n",
            "Episode 4/100 | Reward: -128.4 | Epsilon: 0.980\n",
            "Episode 5/100 | Reward: -260.8 | Epsilon: 0.975\n",
            "Episode 6/100 | Reward: -229.2 | Epsilon: 0.970\n",
            "Episode 7/100 | Reward: -97.0 | Epsilon: 0.966\n",
            "Episode 8/100 | Reward: -249.1 | Epsilon: 0.961\n",
            "Episode 9/100 | Reward: -63.4 | Epsilon: 0.956\n",
            "Episode 10/100 | Reward: -121.2 | Epsilon: 0.951\n",
            "Episode 11/100 | Reward: -192.5 | Epsilon: 0.946\n",
            "Episode 12/100 | Reward: -87.5 | Epsilon: 0.942\n",
            "Episode 13/100 | Reward: -105.4 | Epsilon: 0.937\n",
            "Episode 14/100 | Reward: -47.7 | Epsilon: 0.932\n",
            "Episode 15/100 | Reward: -132.0 | Epsilon: 0.928\n",
            "Episode 16/100 | Reward: -48.7 | Epsilon: 0.923\n",
            "Episode 17/100 | Reward: -36.1 | Epsilon: 0.918\n",
            "Episode 18/100 | Reward: -243.3 | Epsilon: 0.914\n",
            "Episode 19/100 | Reward: -199.4 | Epsilon: 0.909\n",
            "Episode 20/100 | Reward: -129.1 | Epsilon: 0.905\n",
            "Episode 21/100 | Reward: -139.8 | Epsilon: 0.900\n",
            "Episode 22/100 | Reward: -126.8 | Epsilon: 0.896\n",
            "Episode 23/100 | Reward: -157.8 | Epsilon: 0.891\n",
            "Episode 24/100 | Reward: -195.0 | Epsilon: 0.887\n",
            "Episode 25/100 | Reward: -192.4 | Epsilon: 0.882\n",
            "Episode 26/100 | Reward: -47.4 | Epsilon: 0.878\n",
            "Episode 27/100 | Reward: -92.6 | Epsilon: 0.873\n",
            "Episode 28/100 | Reward: -110.2 | Epsilon: 0.869\n",
            "Episode 29/100 | Reward: -98.8 | Epsilon: 0.865\n",
            "Episode 30/100 | Reward: -208.9 | Epsilon: 0.860\n",
            "Episode 31/100 | Reward: -116.5 | Epsilon: 0.856\n",
            "Episode 32/100 | Reward: -292.6 | Epsilon: 0.852\n",
            "Episode 33/100 | Reward: -44.4 | Epsilon: 0.848\n",
            "Episode 34/100 | Reward: -153.6 | Epsilon: 0.843\n",
            "Episode 35/100 | Reward: -160.7 | Epsilon: 0.839\n",
            "Episode 36/100 | Reward: -230.7 | Epsilon: 0.835\n",
            "Episode 37/100 | Reward: -138.2 | Epsilon: 0.831\n",
            "Episode 38/100 | Reward: -77.7 | Epsilon: 0.827\n",
            "Episode 39/100 | Reward: -110.0 | Epsilon: 0.822\n",
            "Episode 40/100 | Reward: -327.7 | Epsilon: 0.818\n",
            "Episode 41/100 | Reward: -116.3 | Epsilon: 0.814\n",
            "Episode 42/100 | Reward: -137.7 | Epsilon: 0.810\n",
            "Episode 43/100 | Reward: -80.2 | Epsilon: 0.806\n",
            "Episode 44/100 | Reward: -100.1 | Epsilon: 0.802\n",
            "Episode 45/100 | Reward: -96.5 | Epsilon: 0.798\n",
            "Episode 46/100 | Reward: -73.0 | Epsilon: 0.794\n",
            "Episode 47/100 | Reward: -120.1 | Epsilon: 0.790\n",
            "Episode 48/100 | Reward: -86.2 | Epsilon: 0.786\n",
            "Episode 49/100 | Reward: -70.2 | Epsilon: 0.782\n",
            "Episode 50/100 | Reward: -103.3 | Epsilon: 0.778\n",
            "Episode 51/100 | Reward: -132.3 | Epsilon: 0.774\n",
            "Episode 52/100 | Reward: -98.5 | Epsilon: 0.771\n",
            "Episode 53/100 | Reward: -50.6 | Epsilon: 0.767\n",
            "Episode 54/100 | Reward: -98.4 | Epsilon: 0.763\n",
            "Episode 55/100 | Reward: -90.6 | Epsilon: 0.759\n",
            "Episode 56/100 | Reward: -160.7 | Epsilon: 0.755\n",
            "Episode 57/100 | Reward: -310.6 | Epsilon: 0.751\n",
            "Episode 58/100 | Reward: -114.2 | Epsilon: 0.748\n",
            "Episode 59/100 | Reward: -128.5 | Epsilon: 0.744\n",
            "Episode 60/100 | Reward: -152.9 | Epsilon: 0.740\n",
            "Episode 61/100 | Reward: -57.3 | Epsilon: 0.737\n",
            "Episode 62/100 | Reward: -57.1 | Epsilon: 0.733\n",
            "Episode 63/100 | Reward: -112.1 | Epsilon: 0.729\n",
            "Episode 64/100 | Reward: -42.7 | Epsilon: 0.726\n",
            "Episode 65/100 | Reward: -25.2 | Epsilon: 0.722\n",
            "Episode 66/100 | Reward: -138.6 | Epsilon: 0.718\n",
            "Episode 67/100 | Reward: -55.2 | Epsilon: 0.715\n",
            "Episode 68/100 | Reward: -42.6 | Epsilon: 0.711\n",
            "Episode 69/100 | Reward: -161.5 | Epsilon: 0.708\n",
            "Episode 70/100 | Reward: -161.3 | Epsilon: 0.704\n",
            "Episode 71/100 | Reward: -137.8 | Epsilon: 0.701\n",
            "Episode 72/100 | Reward: -34.7 | Epsilon: 0.697\n",
            "Episode 73/100 | Reward: -85.5 | Epsilon: 0.694\n",
            "Episode 74/100 | Reward: -78.3 | Epsilon: 0.690\n",
            "Episode 75/100 | Reward: -176.8 | Epsilon: 0.687\n",
            "Episode 76/100 | Reward: -48.6 | Epsilon: 0.683\n",
            "Episode 77/100 | Reward: -67.9 | Epsilon: 0.680\n",
            "Episode 78/100 | Reward: -46.0 | Epsilon: 0.676\n",
            "Episode 79/100 | Reward: -104.8 | Epsilon: 0.673\n",
            "Episode 80/100 | Reward: -3.4 | Epsilon: 0.670\n",
            "Episode 81/100 | Reward: -127.2 | Epsilon: 0.666\n",
            "Episode 82/100 | Reward: -90.2 | Epsilon: 0.663\n",
            "Episode 83/100 | Reward: -323.4 | Epsilon: 0.660\n",
            "Episode 84/100 | Reward: -81.5 | Epsilon: 0.656\n",
            "Episode 85/100 | Reward: -79.5 | Epsilon: 0.653\n",
            "Episode 86/100 | Reward: -73.8 | Epsilon: 0.650\n",
            "Episode 87/100 | Reward: -69.4 | Epsilon: 0.647\n",
            "Episode 88/100 | Reward: -177.3 | Epsilon: 0.643\n",
            "Episode 89/100 | Reward: -76.7 | Epsilon: 0.640\n",
            "Episode 90/100 | Reward: -133.8 | Epsilon: 0.637\n",
            "Episode 91/100 | Reward: -108.1 | Epsilon: 0.634\n",
            "Episode 92/100 | Reward: -106.7 | Epsilon: 0.631\n",
            "Episode 93/100 | Reward: -73.5 | Epsilon: 0.627\n",
            "Episode 94/100 | Reward: -35.8 | Epsilon: 0.624\n",
            "Episode 95/100 | Reward: -84.2 | Epsilon: 0.621\n",
            "Episode 96/100 | Reward: 23.3 | Epsilon: 0.618\n",
            "Episode 97/100 | Reward: -97.0 | Epsilon: 0.615\n",
            "Episode 98/100 | Reward: -49.8 | Epsilon: 0.612\n",
            "Episode 99/100 | Reward: -141.8 | Epsilon: 0.609\n",
            "Episode 100/100 | Reward: -94.4 | Epsilon: 0.606\n"
          ]
        }
      ],
      "source": [
        "rewards = run_episodes(num_episodes=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d60b6e-3530-48d5-b1d0-7f837b5a8c1c",
      "metadata": {
        "id": "94d60b6e-3530-48d5-b1d0-7f837b5a8c1c"
      },
      "source": [
        "#### 6) Run a few episodes and show results\n",
        "**(POINTS: 1 if successful)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa3fffd-f96d-4773-a7bc-862cbf5eb310",
      "metadata": {
        "id": "6fa3fffd-f96d-4773-a7bc-862cbf5eb310"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}